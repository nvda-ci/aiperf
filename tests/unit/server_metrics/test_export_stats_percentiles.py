# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

"""Tests for estimated percentiles computation and edge cases."""

import numpy as np
import pytest

from aiperf.common.constants import NANOS_PER_SECOND
from aiperf.common.models.export_stats import HistogramExportStats
from aiperf.common.models.histogram_analysis import (
    BucketStatistics,
    accumulate_bucket_statistics,
    estimate_bucket_sums,
    extract_all_observations,
    get_bucket_bounds,
    histogram_quantile,
)
from aiperf.common.models.histogram_percentiles import (
    compute_estimated_percentiles,
    generate_observations_with_sum_constraint,
)
from aiperf.common.models.server_metrics_models import ServerMetricsTimeSeries
from tests.unit.server_metrics.helpers import add_histogram_snapshots, hist

# =============================================================================
# Estimated Percentiles Tests
# =============================================================================


class TestComputeEstimatedPercentiles:
    """Test compute_estimated_percentiles function."""

    def test_computes_percentiles_with_inf_observations(self):
        """Test percentile computation including +Inf bucket estimates."""
        # Cumulative: 90 <= 0.5, 97 <= 1.0, 100 total (3 in +Inf)
        bucket_deltas = {"0.5": 90.0, "1.0": 97.0, "+Inf": 100.0}
        bucket_stats = {}  # No learned stats
        total_sum = 100.0
        total_count = 100

        result = compute_estimated_percentiles(
            bucket_deltas, bucket_stats, total_sum, total_count
        )

        assert result is not None
        assert result.p50_estimate is not None
        assert result.p99_estimate is not None

    def test_returns_none_on_empty_data(self):
        """Test returns None when data is empty."""
        result = compute_estimated_percentiles(
            bucket_deltas={},
            bucket_stats={},
            total_sum=0.0,
            total_count=0,
        )

        assert result is None

    def test_zero_sum_with_observations_returns_zero_percentiles(self):
        """Test that zero sum with nonzero count returns all-zero percentiles.

        This handles the case where all observations are exactly 0 (e.g., decode time
        for a prefill-only worker). Without this check, bucket interpolation would
        give misleading non-zero estimates based on bucket boundaries.
        """
        # All 50 observations fall in first bucket (0.3), but sum is 0
        bucket_deltas = {
            "0.3": 50.0,
            "0.5": 50.0,
            "0.8": 50.0,
            "1.0": 50.0,
            "+Inf": 50.0,
        }
        bucket_stats = {}
        total_sum = 0.0  # All observations were exactly 0
        total_count = 50

        result = compute_estimated_percentiles(
            bucket_deltas, bucket_stats, total_sum, total_count
        )

        assert result is not None
        assert result.p50_estimate == 0.0
        assert result.p90_estimate == 0.0
        assert result.p95_estimate == 0.0
        assert result.p99_estimate == 0.0


class TestEstimatedPercentilesIntegration:
    """Integration tests for estimated percentiles in HistogramExportStats."""

    def test_percentile_estimates_computed_in_from_time_series(self):
        """Test that percentile estimates are computed in from_time_series."""
        ts = ServerMetricsTimeSeries()
        add_histogram_snapshots(
            ts,
            "ttft",
            [
                (0, hist({"0.5": 0.0, "1.0": 0.0, "+Inf": 0.0}, 0.0, 0.0)),
                (
                    NANOS_PER_SECOND,
                    hist({"0.5": 50.0, "1.0": 90.0, "+Inf": 100.0}, 70.0, 100.0),
                ),
            ],
        )

        stats = HistogramExportStats.from_time_series(ts.histograms["ttft"])

        assert stats.p50_estimate is not None
        assert stats.p99_estimate is not None

    def test_percentile_estimates_handle_inf_bucket(self):
        """Test percentile estimates include +Inf bucket in tail percentiles."""
        ts = ServerMetricsTimeSeries()
        # Large +Inf bucket to test tail percentile estimation
        add_histogram_snapshots(
            ts,
            "latency",
            [
                (0, hist({"1.0": 0.0, "+Inf": 0.0}, 0.0, 0.0)),
                # 50 observations <= 1.0s, 50 in +Inf (>1.0s) with high values
                (
                    NANOS_PER_SECOND,
                    hist({"1.0": 50.0, "+Inf": 100.0}, 750.0, 100.0),  # avg=7.5s
                ),
            ],
        )

        stats = HistogramExportStats.from_time_series(ts.histograms["latency"])

        # 50 of 100 observations are in +Inf, so p99 (99th percentile) should include +Inf
        assert stats.p99_estimate is not None
        # With 50% in +Inf, the p99 should be in the +Inf region (above 1.0)
        assert stats.p99_estimate > 1.0

    def test_percentile_estimates_none_on_counter_reset(self):
        """Test percentile estimates are None when counter reset detected."""
        ts = ServerMetricsTimeSeries()
        add_histogram_snapshots(
            ts,
            "latency",
            [
                (0, hist({"1.0": 1000.0, "+Inf": 2000.0}, 500.0, 2000.0)),
                # Counter reset
                (NANOS_PER_SECOND, hist({"1.0": 50.0, "+Inf": 100.0}, 25.0, 100.0)),
            ],
        )

        stats = HistogramExportStats.from_time_series(ts.histograms["latency"])

        assert stats.p50_estimate is None
        assert stats.p99_estimate is None


# =============================================================================
# Edge Case Tests
# =============================================================================


class TestEdgeCasesValidation:
    """Test edge cases and input validation."""

    def test_extract_all_observations_mismatched_lengths_raises(self):
        """Test that mismatched array lengths raise ValueError."""
        timestamps = np.array([0, NANOS_PER_SECOND, 2 * NANOS_PER_SECOND])
        counts = np.array([0.0, 1.0])  # Wrong length!
        sums = np.array([0.0, 0.1, 0.2])
        bucket_snapshots = [
            {"0.5": 0.0, "+Inf": 0.0},
            {"0.5": 1.0, "+Inf": 1.0},
            {"0.5": 2.0, "+Inf": 2.0},
        ]

        with pytest.raises(ValueError, match="Array length mismatch"):
            extract_all_observations(timestamps, sums, counts, bucket_snapshots)

    def test_extract_all_observations_mismatched_bucket_snapshots_raises(self):
        """Test that mismatched bucket_snapshots length raises ValueError."""
        timestamps = np.array([0, NANOS_PER_SECOND, 2 * NANOS_PER_SECOND])
        counts = np.array([0.0, 1.0, 2.0])
        sums = np.array([0.0, 0.1, 0.2])
        bucket_snapshots = [  # Wrong length - only 2 instead of 3
            {"0.5": 0.0, "+Inf": 0.0},
            {"0.5": 1.0, "+Inf": 1.0},
        ]

        with pytest.raises(ValueError, match="bucket_snapshots length"):
            extract_all_observations(timestamps, sums, counts, bucket_snapshots)

    def test_accumulate_bucket_statistics_mismatched_lengths_raises(self):
        """Test that accumulate_bucket_statistics validates array lengths."""
        timestamps = np.array([0, NANOS_PER_SECOND])
        counts = np.array([0.0, 1.0, 2.0])  # Wrong length!
        sums = np.array([0.0, 0.1])
        bucket_snapshots = [{"0.5": 0.0}, {"0.5": 1.0}]

        with pytest.raises(ValueError, match="Array length mismatch"):
            accumulate_bucket_statistics(timestamps, sums, counts, bucket_snapshots)

    def test_histogram_quantile_handles_very_large_counts(self):
        """Test histogram_quantile handles very large observation counts."""
        # 1 billion observations - tests for overflow
        buckets = {
            "0.5": 500_000_000.0,
            "1.0": 1_000_000_000.0,
            "+Inf": 1_000_000_000.0,
        }

        p50 = histogram_quantile(0.50, buckets)
        p99 = histogram_quantile(0.99, buckets)

        assert p50 is not None
        assert p99 is not None
        assert 0 < p50 <= 0.5
        assert 0.5 < p99 <= 1.0

    def test_histogram_quantile_handles_very_small_counts(self):
        """Test histogram_quantile handles fractional counts (should still work)."""
        # Fractional counts can happen with rate calculations
        buckets = {"0.5": 0.5, "1.0": 1.0, "+Inf": 1.0}

        p50 = histogram_quantile(0.50, buckets)

        assert p50 is not None
        assert 0 <= p50 <= 0.5

    def test_generate_observations_handles_very_large_bucket(self):
        """Test observation generation handles very wide bucket ranges."""
        # Bucket from 0 to 1000 (very wide)
        bucket_deltas = {"1000.0": 100.0, "+Inf": 100.0}
        target_sum = 50000.0  # mean of 500

        observations = generate_observations_with_sum_constraint(
            bucket_deltas, target_sum=target_sum, bucket_stats=None
        )

        assert len(observations) == 100
        assert all(0 <= obs <= 1000 for obs in observations)

    def test_generate_observations_handles_tiny_bucket(self):
        """Test observation generation handles very narrow bucket ranges."""
        # Bucket from 0 to 0.001 (very narrow)
        bucket_deltas = {"0.001": 100.0, "+Inf": 100.0}
        target_sum = 0.05  # mean of 0.0005

        observations = generate_observations_with_sum_constraint(
            bucket_deltas, target_sum=target_sum, bucket_stats=None
        )

        assert len(observations) == 100
        assert all(0 <= obs <= 0.001 for obs in observations)

    def test_bucket_statistics_handles_zero_count(self):
        """Test BucketStatistics handles zero-count records gracefully."""
        stats = BucketStatistics(bucket_le="0.5")
        stats.record(mean=0.25, count=0)  # Zero count

        assert stats.observation_count == 0
        assert stats.estimated_mean is None  # No valid mean

    def test_estimate_inf_bucket_handles_extreme_values(self):
        """Test +Inf bucket estimation handles extreme sum values."""
        from aiperf.common.models.histogram_analysis import (
            estimate_inf_bucket_observations,
        )

        # Very large sum relative to finite observations
        observations = estimate_inf_bucket_observations(
            total_sum=1_000_000.0,
            estimated_finite_sum=100.0,
            inf_count=10,
            max_finite_bucket=10.0,
        )

        # inf_avg = (1_000_000 - 100) / 10 = 99,990
        assert len(observations) == 10
        assert all(obs >= 10.0 for obs in observations)  # All above max finite

    def test_compute_estimated_percentiles_handles_all_in_inf_bucket(self):
        """Test estimated_percentiles handles case where all observations in +Inf."""
        # All 100 observations in +Inf bucket
        bucket_deltas = {"1.0": 0.0, "+Inf": 100.0}
        bucket_stats = {}
        total_sum = 1500.0  # Average of 15 per observation
        total_count = 100

        result = compute_estimated_percentiles(
            bucket_deltas, bucket_stats, total_sum, total_count
        )

        assert result is not None
        # All percentiles should be in the +Inf region
        assert result.p50_estimate is not None
        assert result.p50_estimate > 1.0


class TestEdgeCasesNumericalStability:
    """Test numerical edge cases for stability."""

    def test_histogram_quantile_with_zero_in_bucket_boundary(self):
        """Test histogram with 0 as bucket boundary works correctly."""
        # Some histograms use 0 as first boundary
        buckets = {"0": 10.0, "0.5": 50.0, "+Inf": 50.0}

        p50 = histogram_quantile(0.50, buckets)

        # p50 should be in the (0, 0.5] bucket
        assert p50 is not None
        assert 0 <= p50 <= 0.5

    def test_bucket_bounds_first_bucket_negative_boundary(self):
        """Test get_bucket_bounds handles negative bucket boundaries."""
        sorted_buckets = ["-1.0", "0", "1.0"]

        lower, upper = get_bucket_bounds("-1.0", sorted_buckets)
        # First bucket: lower bound is typically 0, but for negative buckets
        # the logic may differ
        assert upper == -1.0

    def test_generate_observations_exact_sum_match(self):
        """Test that generated observations match target sum closely."""
        bucket_deltas = {"0.5": 50.0, "1.0": 100.0, "+Inf": 100.0}
        target_sum = 37.5  # Exact midpoint sum: 50*0.25 + 50*0.75 = 12.5 + 37.5 = 50

        observations = generate_observations_with_sum_constraint(
            bucket_deltas, target_sum=target_sum, bucket_stats=None
        )

        # Sum should be close to target (within 5% due to clamping limits)
        assert np.isclose(observations.sum(), target_sum, rtol=0.15)

    def test_estimate_bucket_sums_empty_buckets(self):
        """Test estimate_bucket_sums with empty bucket counts."""
        bucket_deltas = {"0.5": 0.0, "1.0": 0.0, "+Inf": 0.0}
        bucket_stats = {}

        sums = estimate_bucket_sums(bucket_deltas, bucket_stats)

        # All zeros should result in empty or zero sums
        assert all(v == 0.0 for v in sums.values()) or len(sums) == 0
