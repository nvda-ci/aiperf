# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
"""Component integration tests for sticky credit router load balancing.

These tests verify that the StickyCreditRouter:
1. Distributes credits fairly across workers (least-loaded selection)
2. Maintains sticky routing (all turns of a session go to same worker)
3. Handles various worker counts and workload patterns

The router uses random selection among tied workers (same load level)
to achieve fair distribution without round-robin bias.
"""

import pytest

from tests.component_integration.timing.conftest import (
    TimingTestConfig,
    defaults,
)
from tests.harness.analyzers import LoadBalancingAnalyzer
from tests.harness.utils import AIPerfCLI


def build_multi_worker_command(
    config: TimingTestConfig,
    workers_max: int,
    *,
    arrival_pattern: str = "constant",
) -> str:
    """Build command with specified number of workers."""
    cmd = f"""
        aiperf profile \
            --model {defaults.model} \
            --streaming \
            --num-sessions {config.num_sessions} \
            --osl {config.osl} \
            --extra-inputs ignore_eos:true \
            --workers-max {workers_max} \
            --random-seed 42 \
            --ui {defaults.ui}
    """

    if config.turns_per_session > 1:
        cmd += (
            f" --session-turns-mean {config.turns_per_session} --session-turns-stddev 0"
        )

    if config.concurrency is not None:
        cmd += f" --concurrency {config.concurrency}"

    if config.qps > 0:
        cmd += f" --request-rate {config.qps} --request-rate-mode {arrival_pattern}"

    return cmd


def build_burst_multi_worker_command(
    config: TimingTestConfig,
    workers_max: int,
) -> str:
    """Build burst mode command with specified number of workers."""
    cmd = f"""
        aiperf profile \
            --model {defaults.model} \
            --streaming \
            --num-sessions {config.num_sessions} \
            --concurrency {config.concurrency} \
            --osl {config.osl} \
            --extra-inputs ignore_eos:true \
            --workers-max {workers_max} \
            --random-seed 42 \
            --ui {defaults.ui}
    """

    if config.turns_per_session > 1:
        cmd += (
            f" --session-turns-mean {config.turns_per_session} --session-turns-stddev 0"
        )

    return cmd


@pytest.mark.component_integration
class TestFairDistributionSingleTurn:
    """Test fair credit distribution with single-turn sessions.

    Single-turn sessions test pure load balancing without sticky routing effects.
    Each session is a single credit, so distribution should be near-perfect.
    """

    @pytest.mark.parametrize(
        "num_sessions,workers_max,tolerance_pct",
        [
            (100, 2, 20.0),   # 50 credits/worker expected
            (100, 3, 25.0),   # ~33 credits/worker expected
            (100, 5, 30.0),   # 20 credits/worker expected
            pytest.param(200, 3, 20.0, marks=pytest.mark.slow),  # Higher volume
            pytest.param(150, 5, 25.0, marks=pytest.mark.slow),  # 30 credits/worker
        ],
    )  # fmt: skip
    def test_fair_distribution_constant_rate(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        workers_max: int,
        tolerance_pct: float,
    ):
        """Test fair credit distribution with constant rate."""
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(
            config, workers_max, arrival_pattern="constant"
        )
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        assert analyzer.num_workers == workers_max, (
            f"Expected {workers_max} workers, got {analyzer.num_workers}"
        )

        passed, reason = analyzer.verify_fair_distribution(tolerance_pct=tolerance_pct)
        assert passed, reason

    @pytest.mark.parametrize(
        "num_sessions,workers_max,tolerance_pct",
        [
            (100, 2, 25.0),
            (100, 3, 30.0),
            (100, 5, 35.0),
        ],
    )  # fmt: skip
    def test_fair_distribution_poisson_rate(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        workers_max: int,
        tolerance_pct: float,
    ):
        """Test fair distribution with Poisson (random) arrival rate."""
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max, arrival_pattern="poisson")
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        assert analyzer.num_workers == workers_max

        passed, reason = analyzer.verify_fair_distribution(tolerance_pct=tolerance_pct)
        assert passed, reason

    @pytest.mark.parametrize(
        "num_sessions,workers_max,concurrency,tolerance_pct",
        [
            (100, 2, 10, 20.0),
            (100, 3, 15, 25.0),
            (100, 5, 20, 30.0),
        ],
    )  # fmt: skip
    def test_fair_distribution_burst_mode(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        workers_max: int,
        concurrency: int,
        tolerance_pct: float,
    ):
        """Test fair distribution in burst mode (no rate limiting)."""
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=0,
            turns_per_session=1,
            concurrency=concurrency,
        )
        cmd = build_burst_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        assert analyzer.num_workers == workers_max

        passed, reason = analyzer.verify_fair_distribution(tolerance_pct=tolerance_pct)
        assert passed, reason


@pytest.mark.component_integration
class TestStickyRoutingMultiTurn:
    """Test sticky routing for multi-turn sessions.

    Multi-turn sessions must route all turns to the same worker for
    conversation state caching. Verify sticky routing is maintained.
    """

    @pytest.mark.parametrize(
        "num_sessions,turns_per_session,workers_max",
        [
            (20, 3, 2),   # 20 sessions Ã— 3 turns = 60 credits
            (20, 3, 3),
            (20, 5, 3),   # More turns per session
            pytest.param(30, 4, 5, marks=pytest.mark.slow),  # More workers
            (50, 2, 3),   # Many short sessions
        ],
    )  # fmt: skip
    def test_sticky_routing_maintained(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        turns_per_session: int,
        workers_max: int,
    ):
        """Verify all turns of each session go to the same worker."""
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=turns_per_session,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        expected_requests = num_sessions * turns_per_session
        assert result.request_count == expected_requests

        analyzer = LoadBalancingAnalyzer(result)

        # Verify sticky routing
        passed, reason = analyzer.verify_sticky_routing()
        assert passed, reason

        # Verify correct number of sessions per worker
        assert analyzer.total_sessions == num_sessions

    @pytest.mark.parametrize(
        "num_sessions,turns_per_session,workers_max,concurrency",
        [
            (30, 3, 3, 15),
            (40, 4, 5, 20),
        ],
    )  # fmt: skip
    def test_sticky_routing_burst_mode(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        turns_per_session: int,
        workers_max: int,
        concurrency: int,
    ):
        """Verify sticky routing under burst mode (high concurrency)."""
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=0,
            turns_per_session=turns_per_session,
            concurrency=concurrency,
        )
        cmd = build_burst_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        expected_requests = num_sessions * turns_per_session
        assert result.request_count == expected_requests

        analyzer = LoadBalancingAnalyzer(result)
        passed, reason = analyzer.verify_sticky_routing()
        assert passed, reason


@pytest.mark.component_integration
class TestFirstTurnDistribution:
    """Test fair distribution of first turns (new session assignment).

    First turns test the load balancing algorithm directly since
    they create new sticky sessions. Subsequent turns follow sticky routing.
    """

    @pytest.mark.parametrize(
        "num_sessions,turns_per_session,workers_max,tolerance_pct",
        [
            pytest.param(60, 3, 2, 20.0, marks=pytest.mark.slow),
            pytest.param(60, 3, 3, 25.0, marks=pytest.mark.slow),
            pytest.param(60, 5, 3, 25.0, marks=[pytest.mark.stress, pytest.mark.slow]),  # More turns
            pytest.param(100, 2, 5, 30.0, marks=pytest.mark.slow)
        ],
    )  # fmt: skip
    def test_first_turn_fair_distribution(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        turns_per_session: int,
        workers_max: int,
        tolerance_pct: float,
    ):
        """Verify first turns are fairly distributed (independent of sticky effects)."""
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=turns_per_session,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        expected_requests = num_sessions * turns_per_session
        assert result.request_count == expected_requests

        analyzer = LoadBalancingAnalyzer(result)

        # First turns should be evenly distributed
        passed, reason = analyzer.verify_first_turn_distribution(
            tolerance_pct=tolerance_pct
        )
        assert passed, reason

        # Verify sticky routing still works
        sticky_passed, sticky_reason = analyzer.verify_sticky_routing()
        assert sticky_passed, sticky_reason


@pytest.mark.component_integration
class TestMultiWorkerScaling:
    """Test load balancing scales correctly with worker count."""

    @pytest.mark.parametrize(
        "workers_max",
        [2, 3, 5, 7],
    )
    def test_all_workers_receive_credits(
        self,
        cli: AIPerfCLI,
        workers_max: int,
    ):
        """Verify all workers receive at least some credits."""
        # Use enough sessions to ensure all workers get work
        num_sessions = workers_max * 20
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=500.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        assert analyzer.num_workers == workers_max, (
            f"Expected all {workers_max} workers to receive credits, "
            f"but only {analyzer.num_workers} did"
        )

        # Every worker should have received credits
        credits_per_worker = analyzer.credits_per_worker()
        for worker_id, count in credits_per_worker.items():
            assert count > 0, f"Worker {worker_id} received no credits"

    def test_single_worker_gets_all_credits(self, cli: AIPerfCLI):
        """Verify single worker receives all credits."""
        num_sessions = 50
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max=1)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        assert analyzer.num_workers == 1
        assert analyzer.total_credits == num_sessions


@pytest.mark.component_integration
@pytest.mark.stress
@pytest.mark.slow
class TestHighLoadBalancing:
    """Test load balancing under high load conditions."""

    def test_high_session_count_fair_distribution(self, cli: AIPerfCLI):
        """Test fair distribution with many sessions."""
        workers_max = 5
        num_sessions = 500
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=500.0,  # High throughput
            turns_per_session=1,
            timeout=120.0,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)

        # With high volume, distribution should be very fair
        passed, reason = analyzer.verify_fair_distribution(tolerance_pct=15.0)
        assert passed, reason

        # Check distribution stats
        stats = analyzer.get_distribution_stats()
        # Coefficient of variation should be low for fair distribution
        assert stats["cv"] < 0.15, (
            f"Distribution CV {stats['cv']:.3f} too high (expected < 0.15)"
        )

    def test_high_concurrency_fair_distribution(self, cli: AIPerfCLI):
        """Test fair distribution under high concurrency burst."""
        workers_max = 5
        num_sessions = 200
        concurrency = 50
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=0,
            turns_per_session=1,
            concurrency=concurrency,
            timeout=90.0,
        )
        cmd = build_burst_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)

        passed, reason = analyzer.verify_fair_distribution(tolerance_pct=25.0)
        assert passed, reason


@pytest.mark.component_integration
class TestMixedWorkload:
    """Test load balancing with mixed session lengths."""

    @pytest.mark.stress
    @pytest.mark.slow
    def test_multi_turn_fair_total_distribution(self, cli: AIPerfCLI):
        """Test that total credits are fairly distributed with multi-turn sessions.

        Even with sticky routing, total credits should be fairly distributed
        because first turns (which determine session assignment) are fairly distributed.
        """
        workers_max = 3
        num_sessions = 60
        turns_per_session = 4
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=turns_per_session,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        expected_requests = num_sessions * turns_per_session
        assert result.request_count == expected_requests

        analyzer = LoadBalancingAnalyzer(result)

        # Total credits should be fairly distributed
        passed, reason = analyzer.verify_fair_distribution(tolerance_pct=25.0)
        assert passed, reason

        # First turns should be fairly distributed
        first_passed, first_reason = analyzer.verify_first_turn_distribution(
            tolerance_pct=25.0
        )
        assert first_passed, first_reason

        # Sticky routing must be maintained
        sticky_passed, sticky_reason = analyzer.verify_sticky_routing()
        assert sticky_passed, sticky_reason

    @pytest.mark.slow
    def test_sessions_distributed_evenly(self, cli: AIPerfCLI):
        """Test that sessions (not just credits) are evenly distributed."""
        workers_max = 4
        num_sessions = 80
        turns_per_session = 3
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=turns_per_session,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        expected_requests = num_sessions * turns_per_session
        assert result.request_count == expected_requests

        analyzer = LoadBalancingAnalyzer(result)

        # Check session distribution
        sessions_per_worker = analyzer.sessions_per_worker()
        expected_sessions = num_sessions / workers_max
        tolerance = expected_sessions * 0.30  # 30% tolerance

        for worker_id, count in sessions_per_worker.items():
            deviation = abs(count - expected_sessions)
            assert deviation <= tolerance, (
                f"Worker {worker_id} has {count} sessions, "
                f"expected ~{expected_sessions:.1f} (deviation {deviation:.1f} > {tolerance:.1f})"
            )


@pytest.mark.component_integration
class TestDistributionStats:
    """Tests that verify distribution statistics."""

    def test_coefficient_of_variation_acceptable(self, cli: AIPerfCLI):
        """Test that distribution CV is within acceptable range."""
        workers_max = 4
        num_sessions = 200
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=500.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        stats = analyzer.get_distribution_stats()

        # For fair distribution, CV should be low
        # With random selection among tied workers, CV ~ 0.1-0.2 is expected
        assert stats["cv"] < 0.25, (
            f"CV {stats['cv']:.3f} too high for fair distribution"
        )

        # Min and max should be close to mean
        assert stats["min"] >= stats["mean"] * 0.6, (
            f"Min {stats['min']} too far below mean {stats['mean']:.1f}"
        )
        assert stats["max"] <= stats["mean"] * 1.4, (
            f"Max {stats['max']} too far above mean {stats['mean']:.1f}"
        )


@pytest.mark.component_integration
class TestResearchFairnessMetrics:
    """Tests using research-grade fairness metrics.

    These tests use established fairness metrics from computer science
    and economics literature to verify load balancing quality.
    """

    @pytest.mark.parametrize(
        "num_sessions,workers_max,min_jfi",
        [
            (100, 2, 0.95),   # 2 workers should be very fair
            (100, 3, 0.92),   # 3 workers slightly more variance
            (100, 5, 0.90),   # More workers, slightly more variance
            (200, 3, 0.95),   # Higher volume improves fairness
            (200, 5, 0.93),
            pytest.param(500, 5, 0.97, marks=[pytest.mark.stress, pytest.mark.slow])
        ],
    )  # fmt: skip
    def test_jains_fairness_index(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        workers_max: int,
        min_jfi: float,
    ):
        """Test Jain's Fairness Index meets research-grade thresholds.

        JFI of 0.9+ indicates highly fair distribution.
        JFI of 0.95+ indicates near-perfect fairness.
        """
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=500.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        jfi = analyzer.jains_fairness_index()

        assert jfi >= min_jfi, (
            f"Jain's Fairness Index {jfi:.4f} below threshold {min_jfi}. "
            f"Distribution: {analyzer.credits_per_worker()}"
        )

    @pytest.mark.parametrize(
        "num_sessions,workers_max,max_gini",
        [
            (100, 2, 0.10),   # Gini < 0.1 is very equal
            (100, 3, 0.12),
            (100, 5, 0.15),
            (200, 5, 0.10),   # Higher volume, tighter bound
            pytest.param(500, 5, 0.08, marks=[pytest.mark.stress, pytest.mark.slow])
        ],
    )  # fmt: skip
    def test_gini_coefficient(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        workers_max: int,
        max_gini: float,
    ):
        """Test Gini coefficient stays below inequality threshold.

        Gini < 0.1 indicates very equal distribution (like Sweden's income).
        Gini < 0.2 indicates low inequality.
        """
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=500.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        gini = analyzer.gini_coefficient()

        assert gini <= max_gini, (
            f"Gini coefficient {gini:.4f} exceeds threshold {max_gini}. "
            f"Distribution: {analyzer.credits_per_worker()}"
        )

    @pytest.mark.parametrize(
        "num_sessions,workers_max,max_ratio",
        [
            (100, 2, 1.3),    # Max worker has at most 30% more than min
            (100, 3, 1.4),
            (100, 5, 1.5),
            pytest.param(200, 5, 1.3, marks=pytest.mark.slow),  # Higher volume
            pytest.param(500, 5, 1.2, marks=[pytest.mark.stress, pytest.mark.slow])
        ],
    )  # fmt: skip
    def test_max_min_ratio(
        self,
        cli: AIPerfCLI,
        num_sessions: int,
        workers_max: int,
        max_ratio: float,
    ):
        """Test max/min ratio stays within bounds.

        Ratio of 1.0 = perfect balance.
        Ratio of 1.5 = busiest worker did 50% more than least busy.
        """
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=500.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)
        ratio = analyzer.max_min_ratio()

        assert ratio <= max_ratio, (
            f"Max/min ratio {ratio:.3f} exceeds threshold {max_ratio}. "
            f"Distribution: {analyzer.credits_per_worker()}"
        )

    def test_all_fairness_metrics_consistent(self, cli: AIPerfCLI):
        """Test that all fairness metrics agree on a fair distribution."""
        workers_max = 5
        num_sessions = 300
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=500.0,
            turns_per_session=1,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        assert result.request_count == num_sessions

        analyzer = LoadBalancingAnalyzer(result)

        # All metrics should indicate fairness
        jfi = analyzer.jains_fairness_index()
        gini = analyzer.gini_coefficient()
        ratio = analyzer.max_min_ratio()
        stats = analyzer.get_distribution_stats()

        # All should pass reasonable thresholds
        assert jfi >= 0.90, f"JFI {jfi:.4f} too low"
        assert gini <= 0.15, f"Gini {gini:.4f} too high"
        assert ratio <= 1.5, f"Max/min ratio {ratio:.3f} too high"
        assert stats["cv"] < 0.20, f"CV {stats['cv']:.4f} too high"

    @pytest.mark.stress
    @pytest.mark.slow
    def test_multi_turn_jains_fairness(self, cli: AIPerfCLI):
        """Test JFI with multi-turn sessions (sticky routing adds complexity)."""
        workers_max = 4
        num_sessions = 80
        turns_per_session = 4
        config = TimingTestConfig(
            num_sessions=num_sessions,
            qps=400.0,
            turns_per_session=turns_per_session,
        )
        cmd = build_multi_worker_command(config, workers_max)
        result = cli.run_sync(cmd, timeout=config.timeout)

        expected_requests = num_sessions * turns_per_session
        assert result.request_count == expected_requests

        analyzer = LoadBalancingAnalyzer(result)

        # Sticky routing maintains fairness because first turns are fair
        jfi = analyzer.jains_fairness_index()
        assert jfi >= 0.85, (
            f"JFI {jfi:.4f} too low for multi-turn. "
            f"Distribution: {analyzer.credits_per_worker()}"
        )

        # Sticky routing must still work
        sticky_passed, sticky_reason = analyzer.verify_sticky_routing()
        assert sticky_passed, sticky_reason
