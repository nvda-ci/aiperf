# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
"""Synthesizer for generating synthetic traces with prefix patterns."""

from pathlib import Path
from typing import Any

import orjson

from aiperf.common import random_generator as rng
from aiperf.common.mixins import AIPerfLoggerMixin
from aiperf.dataset.synthesis.empirical_sampler import EmpiricalSampler
from aiperf.dataset.synthesis.models import SynthesisParams
from aiperf.dataset.synthesis.prefix_analyzer import PrefixAnalyzer
from aiperf.dataset.synthesis.radix_tree import RadixTree


class Synthesizer(AIPerfLoggerMixin):
    """Generates synthetic traces preserving prefix-sharing patterns.

    Reads an input trace file, builds a radix tree of prefix patterns,
    and generates new synthetic traces with configurable multipliers
    for controlling prefix reuse characteristics.
    """

    def __init__(self, params: SynthesisParams | None = None) -> None:
        """Initialize the synthesizer.

        Args:
            params: SynthesisParams with generation configuration.
                   If None, uses defaults.
        """
        super().__init__(config=None, tokenizer=None)
        self.params = params or SynthesisParams()
        self._tree = RadixTree()
        self._isl_sampler: EmpiricalSampler | None = None
        self._osl_sampler: EmpiricalSampler | None = None
        self._rng = rng.derive("dataset.synthesis.synthesizer")
        self._max_hash_id: int = 0  # Track max hash_id for root multiplier offsets

    def synthesize_from_file(self, trace_file: Path | str) -> list[dict]:
        """Synthesize traces from an input trace file.

        Args:
            trace_file: Path to input JSONL trace file.

        Returns:
            List of synthetic trace dictionaries in mooncake format.
        """
        trace_file = Path(trace_file)

        traces = []
        with open(trace_file) as f:
            for line in f:
                if line.strip():
                    traces.append(orjson.loads(line))

        return self.synthesize_traces(traces)

    def synthesize_traces(self, traces: list[dict]) -> list[dict]:
        """Synthesize traces from a list of trace dictionaries.

        Args:
            traces: List of input trace dictionaries.

        Returns:
            List of synthetic trace dictionaries.
        """
        # First pass: analyze input traces and build tree
        analyzer = PrefixAnalyzer(block_size=self.params.block_size)
        stats = analyzer.analyze_traces(traces)

        self.info(f"Input trace statistics: {stats.total_requests} traces")
        self.info(
            f"ISL range: {stats.min_isl}-{stats.max_isl}, avg: {stats.avg_isl:.1f}"
        )
        self.info(f"Cache hit rate: {stats.cache_hit_rate:.2%}")

        # Build samplers from input distributions
        isls = [t.get("input_length", 0) for t in traces if "input_length" in t]
        osls = [t.get("output_length", 0) for t in traces if "output_length" in t]

        self._isl_sampler = EmpiricalSampler(isls) if isls else None
        self._osl_sampler = EmpiricalSampler(osls) if osls else None

        # Build radix tree from input traces and find max hash_id
        self._max_hash_id = 0
        max_last_hash_id = 0  # Track max hash_ids[-1] (where new IDs start)
        max_trace_len = 0  # Track max hash_ids length
        for trace in traces:
            hash_ids = trace.get("hash_ids", [])
            if hash_ids:
                self._tree.add_path(hash_ids)
                self._max_hash_id = max(self._max_hash_id, max(hash_ids))
                max_last_hash_id = max(max_last_hash_id, hash_ids[-1])
                max_trace_len = max(max_trace_len, len(hash_ids))

        # Adjust _max_hash_id to account for hash IDs generated by multipliers.
        # This ensures root multiplier offsets don't cause collisions between trees.
        if self.params.prefix_root_multiplier > 1 and max_trace_len > 0:
            max_new_ids = 0
            if self.params.prefix_len_multiplier > 1.0:
                max_new_ids += int(
                    max_trace_len * (self.params.prefix_len_multiplier - 1)
                )
            if self.params.prompt_len_multiplier > 1.0:
                max_new_ids += int(self.params.prompt_len_multiplier - 1)
            if max_new_ids > 0:
                # Effective max considers highest starting point plus all additions
                self._max_hash_id = max(
                    self._max_hash_id, max_last_hash_id + max_new_ids
                )

        # Second pass: generate synthetic traces
        synthetic_traces = []
        for trace in traces:
            hash_ids = trace.get("hash_ids", [])
            original_isl = trace.get("input_length", 0)

            # Generate new hash_ids and input_length based on parameters
            if hash_ids:
                new_hash_ids, isl = self._apply_multipliers(hash_ids, original_isl)
            else:
                new_hash_ids = []
                # Sample ISL from distribution when no hash_ids
                isl = self._sample_isl()

            # Sample OSL from distribution
            osl = self._sample_osl()

            # Apply max_isl filter
            if self.params.max_isl and isl > self.params.max_isl:
                isl = self.params.max_isl

            # Apply timestamp scaling if present
            timestamp = trace.get("timestamp")
            if timestamp is not None and self.params.speedup_ratio > 0:
                timestamp = int(timestamp / self.params.speedup_ratio)

            synthetic_trace: dict[str, Any] = {
                "input_length": isl,
                "output_length": osl,
            }

            if new_hash_ids:
                synthetic_trace["hash_ids"] = new_hash_ids

            if timestamp is not None:
                synthetic_trace["timestamp"] = timestamp

            # Preserve session_id and delay if present
            if "session_id" in trace:
                synthetic_trace["session_id"] = trace["session_id"]
            if "delay" in trace:
                synthetic_trace["delay"] = trace["delay"]

            synthetic_traces.append(synthetic_trace)

        self.info(f"Generated {len(synthetic_traces)} synthetic traces")
        return synthetic_traces

    def synthesize_grouped_traces(
        self, data: dict[str, list[dict]]
    ) -> dict[str, list[dict]]:
        """Synthesize traces while preserving session grouping.

        Args:
            data: Dictionary mapping session_id to list of trace dicts.

        Returns:
            Dictionary mapping session_id to list of synthesized trace dicts.
        """
        # Flatten with session_id embedded
        traces = [
            {**trace, "session_id": session_id}
            for session_id, session_traces in data.items()
            for trace in session_traces
        ]

        synthesized = self.synthesize_traces(traces)

        # Re-group by session_id (preserve empty sessions from input)
        result: dict[str, list[dict]] = {sid: [] for sid in data}
        for trace in synthesized:
            session_id = trace.pop("session_id", "default")
            result.setdefault(session_id, []).append(trace)

        return result

    def _apply_multipliers(
        self, hash_ids: list[int], input_length: int
    ) -> tuple[list[int], int]:
        """Apply multiplier transformations to hash IDs and input length.

        When extending hash_ids, properly handles incomplete final blocks:
        - The original incomplete block becomes complete
        - New blocks are added (complete except the final one)
        - The new final block preserves the original's incomplete portion

        New hash IDs are generated as consecutive integers continuing from
        the last hash ID in the specific list. Two hash_ids sharing the same
        integers represents prefix overlap.

        Args:
            hash_ids: Original hash IDs from trace.
            input_length: Original input length in tokens.

        Returns:
            Tuple of (transformed hash_ids, new input_length).
        """
        if not hash_ids:
            return [], input_length

        block_size = self.params.block_size

        # Calculate original incomplete block size (1 to block_size)
        incomplete_tokens = input_length % block_size
        if incomplete_tokens == 0:
            incomplete_tokens = block_size  # Last block was complete

        # Start with original hash_ids (preserves prefix overlap)
        scaled_ids = hash_ids[:]
        new_input_length = input_length
        # Next ID continues from the last one in this specific list (not the max)
        next_id = hash_ids[-1] + 1

        if self.params.prefix_len_multiplier != 1.0:
            scale = self.params.prefix_len_multiplier
            if scale > 1.0:
                # Extend prefix with NEW consecutive hash IDs
                num_to_add = int(len(hash_ids) * (scale - 1))
                if num_to_add > 0:
                    # Complete the original final block
                    tokens_to_complete = block_size - incomplete_tokens
                    new_input_length += tokens_to_complete

                    # Add new complete blocks (all but the last)
                    for _ in range(num_to_add - 1):
                        scaled_ids.append(next_id)
                        next_id += 1
                        new_input_length += block_size

                    # Add final new block (incomplete, matching original pattern)
                    scaled_ids.append(next_id)
                    next_id += 1
                    new_input_length += incomplete_tokens
            elif scale < 1.0:
                # Shorten prefix
                num_to_keep = max(1, int(len(scaled_ids) * scale))
                scaled_ids = scaled_ids[:num_to_keep]
                # Preserve incomplete portion for new final block
                new_input_length = (num_to_keep - 1) * block_size + incomplete_tokens

        # Apply prompt length multiplier (affects unique prompt portion)
        if self.params.prompt_len_multiplier != 1.0 and scaled_ids:
            prompt_scale = self.params.prompt_len_multiplier
            if prompt_scale > 1.0:
                num_to_add = int(prompt_scale - 1)
                if num_to_add > 0:
                    # Recalculate current incomplete portion
                    current_incomplete = new_input_length % block_size
                    if current_incomplete == 0:
                        current_incomplete = block_size

                    # Complete current final block
                    tokens_to_complete = block_size - current_incomplete
                    new_input_length += tokens_to_complete

                    # Add new complete blocks (all but the last)
                    for _ in range(num_to_add - 1):
                        scaled_ids.append(next_id)
                        next_id += 1
                        new_input_length += block_size

                    # Add final new block (incomplete)
                    scaled_ids.append(next_id)
                    next_id += 1
                    new_input_length += current_incomplete

        # Apply root multiplier (randomly assigns trace to one of N independent trees)
        # Each tree has offset = tree_index * (max_hash_id + 1) to avoid collisions
        # Note: This doesn't change input_length or hash_ids length
        if self.params.prefix_root_multiplier > 1 and scaled_ids:
            # Randomly select which tree this trace belongs to (0 to N-1)
            tree_index = self._rng.randint(0, self.params.prefix_root_multiplier - 1)
            if tree_index > 0:
                # Offset all hash_ids to move to a different tree
                offset = tree_index * (self._max_hash_id + 1)
                scaled_ids = [h + offset for h in scaled_ids]

        return scaled_ids, new_input_length

    def _sample_isl(self) -> int:
        """Sample input sequence length from learned distribution.

        Returns:
            Sampled ISL.
        """
        if self._isl_sampler:
            return int(self._isl_sampler.sample())
        return 512

    def _sample_osl(self) -> int:
        """Sample output sequence length from learned distribution.

        Returns:
            Sampled OSL.
        """
        if self._osl_sampler:
            return int(self._osl_sampler.sample())
        return 64

    def get_stats(self) -> dict[str, Any]:
        """Get synthesizer statistics.

        Returns:
            Dictionary with synthesis stats.
        """
        return {
            "tree_nodes": len(self._tree.get_all_nodes()),
            "tree_depth": self._tree.get_stats().max_depth,
            "params": self.params.model_dump(),
        }
